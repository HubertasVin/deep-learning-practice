{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsrFQ+JDIa0OUEIUsajYZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HubertasVin/deep-learning-practice/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importuojamos reikalingos bibliotekos"
      ],
      "metadata": {
        "id": "iRWxyW2qV9II"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SavZT7mEJteq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842e531c-0b8b-43b2-d9af-b18afab8ce2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API imports\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/colab_content'\n",
        "OI_DATA_DIR = DATA_DIR + \"/OpenImages\"\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-6\n",
        "TRAIN_MODE = True  # Toggle: True to train a new model, False to load saved model\n",
        "NGROK_AUTH_TOKEN = \"2v8l4mW8zOFneFzX7p47qORuEwS_4EcRU64N6iq1mUNEpXN3G\"\n",
        "API_ENABLE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duomenų įkėlimas ir paruošimas"
      ],
      "metadata": {
        "id": "ojsetMNbV_SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths_and_labels(root_dir):\n",
        "    classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for cls in classes:\n",
        "        images_dir = os.path.join(root_dir, cls, \"images\")\n",
        "        if os.path.isdir(images_dir):\n",
        "            for file in os.listdir(images_dir):\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(os.path.join(images_dir, file))\n",
        "                    labels.append(class_to_idx[cls])\n",
        "        else:\n",
        "            print(f\"Warning: {images_dir} not found.\")\n",
        "    return image_paths, labels, classes, class_to_idx\n",
        "\n",
        "# Get file paths and labels\n",
        "image_paths, labels, classes, class_to_idx = get_image_paths_and_labels(OI_DATA_DIR)\n",
        "print(\"Total images:\", len(image_paths))\n",
        "print(\"Classes found:\", classes)\n",
        "\n",
        "# Shuffle and split into training (80%) and validation (20%) sets\n",
        "indices = np.arange(len(image_paths))\n",
        "np.random.shuffle(indices)\n",
        "split = int(0.8 * len(image_paths))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_paths = [image_paths[i] for i in train_idx]\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "val_paths   = [image_paths[i] for i in val_idx]\n",
        "val_labels  = [labels[i] for i in val_idx]\n",
        "\n",
        "# Define image processing function\n",
        "def load_and_preprocess_image(path, label, training=True):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Scale to [0,1]\n",
        "    if training:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "    return img, label\n",
        "\n",
        "# Create tf.data.Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(lambda p, l: load_and_preprocess_image(p, l, training=True),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(lambda p, l: load_and_preprocess_image(p, l, training=False),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "bTingvrgV_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelio kompiliavimas ir treniravimas"
      ],
      "metadata": {
        "id": "1NuOmCoZWEnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_path = os.path.join(DATA_DIR, \"keras_model.h5\")\n",
        "\n",
        "if TRAIN_MODE:\n",
        "    history = model.fit(train_ds,\n",
        "                        epochs=EPOCHS,\n",
        "                        validation_data=val_ds)\n",
        "    model.save(model_path)\n",
        "else:\n",
        "    model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "qZ_8eLUsWGiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nupiešti treniravimo ir validacijos nuostolių grafiką"
      ],
      "metadata": {
        "id": "qWGFtJmlH3do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_MODE:\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "x_9sEeTQIUu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pagalbinė prognozavimo funkcija"
      ],
      "metadata": {
        "id": "MdcDTr4TWH-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image):\n",
        "    # Resize and preprocess\n",
        "    img = image.resize(IMG_SIZE)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "    preds = model.predict(img)\n",
        "    pred_class = np.argmax(preds, axis=1)[0]\n",
        "    # Reverse mapping: index -> class name\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "    return idx_to_class.get(pred_class, \"Unknown\")"
      ],
      "metadata": {
        "id": "vx6WpVueIjHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API skirtas įvesti nuotraukas rankiniu būdu.\n",
        "\n",
        "Nuotraukos testavimui su API:\n",
        "- https://img.freepik.com/free-photo/yummy-strawberries-red-mellow-ripe-with-green-leafs-dark-desk_179666-391.jpg\n",
        "- https://img.freepik.com/free-photo/strawberry-isolated-white-background_1232-1974.jpg\n",
        "- https://img.freepik.com/free-photo/traffic-light-city-streets_23-2149091964.jpg\n",
        "- https://img.freepik.com/free-photo/green-traffic-light-intersection_53876-153444.jpg\n",
        "- https://img.freepik.com/premium-photo/close-up-yellow-street-traffic-light-hanging-from-pole_1048944-23361333.jpg\n",
        "- https://img.freepik.com/free-photo/summer-slipper-white-shoes-sandals_1203-6528.jpg\n",
        "- https://img.freepik.com/premium-photo/close-up-shoes-sand_1048944-30578092.jpg\n",
        "- https://img.freepik.com/free-vector/man-brown-casual-flip-flop-sandal-shoes-vector_53876-25895.jpg"
      ],
      "metadata": {
        "id": "bb177y8FqPlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/test', methods=['GET'])\n",
        "def test_endpoint():\n",
        "    return jsonify({\"message\": \"Test endpoint working!\"}), 200\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Allow file upload (multipart/form-data) or JSON with 'url' or 'file_path'\n",
        "    if \"file\" in request.files:\n",
        "        file = request.files[\"file\"]\n",
        "        try:\n",
        "            image = tf.keras.preprocessing.image.load_img(file, target_size=IMG_SIZE)\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": \"Invalid image file.\"}), 400\n",
        "    else:\n",
        "        data = request.get_json(force=True)\n",
        "        if \"url\" in data:\n",
        "            response = requests.get(data[\"url\"])\n",
        "            image = tf.keras.preprocessing.image.load_img(BytesIO(response.content), target_size=IMG_SIZE)\n",
        "        elif \"file_path\" in data:\n",
        "            image = tf.keras.preprocessing.image.load_img(data[\"file_path\"], target_size=IMG_SIZE)\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Provide 'url', 'file_path', or upload a file as 'file'.\"}), 400\n",
        "\n",
        "    prediction = predict_image(image)\n",
        "    return jsonify({\"prediction\": prediction})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    # If run with argument 'api', then start the API server\n",
        "    if API_ENABLE:\n",
        "        # Set the ngrok authtoken\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        public_url = ngrok.connect(5000)\n",
        "        print(\"Public URL:\", public_url)\n",
        "        app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "M1PVSvtxqP39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}