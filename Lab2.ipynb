{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6cl8Svjlj8M3Mwtj/J71m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HubertasVin/deep-learning-practice/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importuojamos reikalingos bibliotekos"
      ],
      "metadata": {
        "id": "iRWxyW2qV9II"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SavZT7mEJteq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74f6d48-b51d-4e6b-f5a7-b1efa34bb0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API imports\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/colab_content'\n",
        "OI_DATA_DIR = DATA_DIR + \"/OpenImages\"\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "LEARNING_RATE = 1e-6\n",
        "TRAIN_MODE = True  # Toggle: True to train a new model, False to load saved model\n",
        "NGROK_AUTH_TOKEN = \"2v8l4mW8zOFneFzX7p47qORuEwS_4EcRU64N6iq1mUNEpXN3G\"\n",
        "API_ENABLE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duomenų įkėlimas ir paruošimas"
      ],
      "metadata": {
        "id": "ojsetMNbV_SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths_and_labels(root_dir):\n",
        "    classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for cls in classes:\n",
        "        images_dir = os.path.join(root_dir, cls, \"images\")\n",
        "        if os.path.isdir(images_dir):\n",
        "            for file in os.listdir(images_dir):\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(os.path.join(images_dir, file))\n",
        "                    labels.append(class_to_idx[cls])\n",
        "        else:\n",
        "            print(f\"Warning: {images_dir} not found.\")\n",
        "    return image_paths, labels, classes, class_to_idx\n",
        "\n",
        "# Get file paths and labels\n",
        "image_paths, labels, classes, class_to_idx = get_image_paths_and_labels(OI_DATA_DIR)\n",
        "print(\"Total images:\", len(image_paths))\n",
        "print(\"Classes found:\", classes)\n",
        "\n",
        "# Shuffle and split into training (80%) and validation (20%) sets\n",
        "indices = np.arange(len(image_paths))\n",
        "np.random.shuffle(indices)\n",
        "split = int(0.8 * len(image_paths))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_paths = [image_paths[i] for i in train_idx]\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "val_paths   = [image_paths[i] for i in val_idx]\n",
        "val_labels  = [labels[i] for i in val_idx]\n",
        "\n",
        "# Define image processing function\n",
        "def load_and_preprocess_image(path, label, training=True):\n",
        "    img = tf.io.read_file(path)\n",
        "    # Use decode_image to support various image formats and force 3 channels\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    # Explicitly set the shape to [None, None, 3]\n",
        "    img.set_shape([None, None, 3])\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Scale to [0,1]\n",
        "    if training:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "# Create tf.data.Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(lambda p, l: load_and_preprocess_image(p, l, training=True),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(lambda p, l: load_and_preprocess_image(p, l, training=False),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTingvrgV_kB",
        "outputId": "743103dc-34e0-4cb2-a7d8-90941c5a8800"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 1061\n",
            "Classes found: ['sandal', 'strawberry', 'traffic light']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelio kompiliavimas ir treniravimas"
      ],
      "metadata": {
        "id": "1NuOmCoZWEnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SIZE + (3,),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "outputs = layers.Dense(len(classes), activation='softmax')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_path = os.path.join(DATA_DIR, \"keras_model.h5\")\n",
        "\n",
        "if TRAIN_MODE:\n",
        "    history = model.fit(train_ds,\n",
        "                        epochs=EPOCHS,\n",
        "                        validation_data=val_ds)\n",
        "    model.save(model_path)\n",
        "else:\n",
        "    model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "qZ_8eLUsWGiK",
        "outputId": "1943480d-c3ce-4323-bee5-64b3e4fed272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ true_divide_4 (\u001b[38;5;33mTrueDivide\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ subtract_4 (\u001b[38;5;33mSubtract\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mobilenetv2_1.00_128 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m3,843\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ true_divide_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ subtract_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mobilenetv2_1.00_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,261,827\u001b[0m (8.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,827</span> (8.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,843\u001b[0m (15.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> (15.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m15/27\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 544ms/step - accuracy: 0.2466 - loss: 1.3440"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nupiešti treniravimo ir validacijos nuostolių grafiką"
      ],
      "metadata": {
        "id": "qWGFtJmlH3do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_MODE:\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "x_9sEeTQIUu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pagalbinė prognozavimo funkcija"
      ],
      "metadata": {
        "id": "MdcDTr4TWH-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image):\n",
        "    # Resize and preprocess\n",
        "    img = image.resize(IMG_SIZE)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "    preds = model.predict(img)\n",
        "    pred_class = np.argmax(preds, axis=1)[0]\n",
        "    # Reverse mapping: index -> class name\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "    return idx_to_class.get(pred_class, \"Unknown\")"
      ],
      "metadata": {
        "id": "vx6WpVueIjHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API skirtas įvesti nuotraukas rankiniu būdu.\n",
        "\n",
        "Nuotraukos testavimui su API:\n",
        "- https://img.freepik.com/free-photo/yummy-strawberries-red-mellow-ripe-with-green-leafs-dark-desk_179666-391.jpg\n",
        "- https://img.freepik.com/free-photo/strawberry-isolated-white-background_1232-1974.jpg\n",
        "- https://img.freepik.com/free-photo/traffic-light-city-streets_23-2149091964.jpg\n",
        "- https://img.freepik.com/free-photo/green-traffic-light-intersection_53876-153444.jpg\n",
        "- https://img.freepik.com/premium-photo/close-up-yellow-street-traffic-light-hanging-from-pole_1048944-23361333.jpg\n",
        "- https://img.freepik.com/free-photo/summer-slipper-white-shoes-sandals_1203-6528.jpg\n",
        "- https://img.freepik.com/premium-photo/close-up-shoes-sand_1048944-30578092.jpg\n",
        "- https://img.freepik.com/free-vector/man-brown-casual-flip-flop-sandal-shoes-vector_53876-25895.jpg"
      ],
      "metadata": {
        "id": "bb177y8FqPlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/test', methods=['GET'])\n",
        "def test_endpoint():\n",
        "    return jsonify({\"message\": \"Test endpoint working!\"}), 200\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Allow file upload (multipart/form-data) or JSON with 'url' or 'file_path'\n",
        "    if \"file\" in request.files:\n",
        "        file = request.files[\"file\"]\n",
        "        try:\n",
        "            image = tf.keras.preprocessing.image.load_img(file, target_size=IMG_SIZE)\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": \"Invalid image file.\"}), 400\n",
        "    else:\n",
        "        data = request.get_json(force=True)\n",
        "        if \"url\" in data:\n",
        "            response = requests.get(data[\"url\"])\n",
        "            image = tf.keras.preprocessing.image.load_img(BytesIO(response.content), target_size=IMG_SIZE)\n",
        "        elif \"file_path\" in data:\n",
        "            image = tf.keras.preprocessing.image.load_img(data[\"file_path\"], target_size=IMG_SIZE)\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Provide 'url', 'file_path', or upload a file as 'file'.\"}), 400\n",
        "\n",
        "    prediction = predict_image(image)\n",
        "    return jsonify({\"prediction\": prediction})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    # If run with argument 'api', then start the API server\n",
        "    if API_ENABLE:\n",
        "        # Set the ngrok authtoken\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        public_url = ngrok.connect(5000)\n",
        "        print(\"Public URL:\", public_url)\n",
        "        app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "M1PVSvtxqP39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}