{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si_ttXBiI1cf"
      },
      "source": [
        "#Dependecies, Image and masks downloads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9O2GjDqzYe9",
        "outputId": "e4c8a514-59c2-4247-fb90-0ca7a4b48ab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet fiftyone pycocotools albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt6g9__3Vp5k"
      },
      "source": [
        "download & split 70 / 15 / 15 % with FiftyOne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv6-SxcJU1ol",
        "outputId": "2e35538e-27c5-4407-d860-56d5e9203968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'train' to 'data_raw/open-images-v7/train' if necessary\n",
            "Necessary images already downloaded\n",
            "Existing download of split 'train' is sufficient\n",
            "Loading 'open-images-v7' split 'train'\n",
            " 100% |█████████████████| 500/500 [10.2s elapsed, 0s remaining, 45.5 samples/s]      \n",
            "Dataset 'banana_tmp' created\n",
            " 100% |█████████████████| 350/350 [1.2s elapsed, 0s remaining, 298.9 samples/s]         \n",
            " 100% |███████████████████| 75/75 [208.6ms elapsed, 0s remaining, 359.6 samples/s]    \n",
            " 100% |███████████████████| 75/75 [242.4ms elapsed, 0s remaining, 309.4 samples/s]    \n",
            "Downloading split 'train' to 'data_raw/open-images-v7/train' if necessary\n",
            "Necessary images already downloaded\n",
            "Existing download of split 'train' is sufficient\n",
            "Loading 'open-images-v7' split 'train'\n",
            " 100% |█████████████████| 500/500 [32.6s elapsed, 0s remaining, 18.9 samples/s]      \n",
            "Dataset 'orange_tmp' created\n",
            " 100% |█████████████████| 350/350 [1.8s elapsed, 0s remaining, 192.2 samples/s]         \n",
            " 100% |███████████████████| 75/75 [410.2ms elapsed, 0s remaining, 182.8 samples/s]      \n",
            " 100% |███████████████████| 75/75 [270.0ms elapsed, 0s remaining, 277.8 samples/s]     \n",
            "Downloading split 'train' to 'data_raw/open-images-v7/train' if necessary\n",
            "Necessary images already downloaded\n",
            "Existing download of split 'train' is sufficient\n",
            "Loading 'open-images-v7' split 'train'\n",
            " 100% |█████████████████| 500/500 [29.5s elapsed, 0s remaining, 18.6 samples/s]      \n",
            "Dataset 'strawberry_tmp' created\n",
            " 100% |█████████████████| 350/350 [1.7s elapsed, 0s remaining, 202.0 samples/s]         \n",
            " 100% |███████████████████| 75/75 [352.6ms elapsed, 0s remaining, 213.4 samples/s]      \n",
            " 100% |███████████████████| 75/75 [473.4ms elapsed, 0s remaining, 158.4 samples/s]      \n",
            "train counts: {'Banana': 597, 'Orange': 1929, 'Strawberry': 1708}\n",
            "val counts: {'Banana': 104, 'Orange': 435, 'Strawberry': 366}\n",
            "test counts: {'Banana': 114, 'Orange': 254, 'Strawberry': 500}\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo, random, os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "CLASSES  = [\"Banana\", \"Orange\", \"Strawberry\"]\n",
        "BUFFERS  = 500\n",
        "DATA_RAW = \"data_raw\"\n",
        "fo.config.dataset_zoo_dir = DATA_RAW\n",
        "\n",
        "train_ds = fo.Dataset(\"fruit_train\", overwrite=True)\n",
        "val_ds   = fo.Dataset(\"fruit_val\",   overwrite=True)\n",
        "test_ds  = fo.Dataset(\"fruit_test\",  overwrite=True)\n",
        "\n",
        "def grab(cls):\n",
        "    return fo.zoo.load_zoo_dataset(\n",
        "        \"open-images-v7\",\n",
        "        split=\"train\",\n",
        "        classes=[cls],\n",
        "        label_types=[\"segmentations\"],\n",
        "        only_matching=True,\n",
        "        max_samples=BUFFERS,\n",
        "        dataset_name=f\"{cls.lower()}_tmp\",\n",
        "    )\n",
        "\n",
        "for cls in CLASSES:\n",
        "    ds  = grab(cls)\n",
        "    ids = ds.values(\"id\"); random.shuffle(ids)\n",
        "    # 70 / 15 / 15 %\n",
        "    n = len(ids); t, v = int(0.70*n), int(0.85*n)\n",
        "    train_ds.add_samples(ds.select(ids[:t]))\n",
        "    val_ds  .add_samples(ds.select(ids[t:v]))\n",
        "    test_ds .add_samples(ds.select(ids[v:]))\n",
        "\n",
        "print(\"train counts:\", train_ds.count_values(\"ground_truth.detections.label\"))\n",
        "print(\"val counts:\", val_ds.count_values(\"ground_truth.detections.label\"))\n",
        "print(\"test counts:\", test_ds.count_values(\"ground_truth.detections.label\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KKaeVi9Vn2R"
      },
      "source": [
        "polygons conversion to semantic PNG masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUchCm0oVV-f",
        "outputId": "594c7d73-17b2-45c2-da6b-892a48a560ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing metadata...\n",
            " 100% |███████████████| 1050/1050 [245.6ms elapsed, 0s remaining, 4.3K samples/s]     \n",
            " 100% |███████████████| 1050/1050 [16.7s elapsed, 0s remaining, 65.1 samples/s]      \n",
            "Computing metadata...\n",
            " 100% |█████████████████| 225/225 [37.8ms elapsed, 0s remaining, 5.9K samples/s] \n",
            " 100% |█████████████████| 225/225 [3.4s elapsed, 0s remaining, 64.8 samples/s]      \n",
            "Computing metadata...\n",
            " 100% |█████████████████| 225/225 [38.0ms elapsed, 0s remaining, 5.9K samples/s] \n",
            " 100% |█████████████████| 225/225 [3.5s elapsed, 0s remaining, 60.6 samples/s]      \n",
            "semantic masks + images written to seg_data_fruit\n"
          ]
        }
      ],
      "source": [
        "import fiftyone.utils.labels as fou, shutil\n",
        "\n",
        "PROC_ROOT   = \"seg_data_fruit\"\n",
        "MASK_TARGET = {i+1: c for i, c in enumerate(CLASSES)}\n",
        "\n",
        "def export(ds, split):\n",
        "    out = f\"{PROC_ROOT}/{split}\"\n",
        "    img_dir, msk_dir = f\"{out}/images\", f\"{out}/masks\"\n",
        "    os.makedirs(img_dir, exist_ok=True); os.makedirs(msk_dir, exist_ok=True)\n",
        "\n",
        "    fou.objects_to_segmentations(\n",
        "        ds,\n",
        "        in_field=\"ground_truth\",\n",
        "        mask_targets=MASK_TARGET,\n",
        "        out_field=\"sem_mask\",\n",
        "        output_dir=msk_dir,\n",
        "        rel_dir=\".\",\n",
        "        overwrite=True,\n",
        "    )\n",
        "    for s in ds:\n",
        "        shutil.copy2(s.filepath, f\"{img_dir}/{os.path.basename(s.filepath)}\")\n",
        "\n",
        "for name, fo_ds in zip((\"train\",\"val\",\"test\"), (train_ds, val_ds, test_ds)):\n",
        "    export(fo_ds, name)\n",
        "\n",
        "print(\"semantic masks + images written to\", PROC_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGFp0O4pViQZ"
      },
      "source": [
        "Back-up to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt59T62eVhQ-",
        "outputId": "2d89526d-c739-43f8-84b5-0b8b324322b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ dataset copied to drive/GMM3\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import shutil, os\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "PROJ_DIR = \"drive/GMM3\"\n",
        "os.makedirs(PROJ_DIR, exist_ok=True)\n",
        "shutil.copytree(PROC_ROOT, f\"{PROJ_DIR}/seg_data\", dirs_exist_ok=True)\n",
        "\n",
        "print(\"✓ dataset copied to\", PROJ_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzV4HK2mJC4I"
      },
      "source": [
        "Move downloaded masks and images to google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtxIjN-VD8h9"
      },
      "source": [
        "#Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXyY04QkD_Bd",
        "outputId": "21742def-371a-47e3-8176-ed5e153141f7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m\"\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import glob, os, cv2, torch, numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "DATA_ROOT = \"drive/GMM3/seg_data\"\n",
        "IMG_SIZE  = 256\n",
        "\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, split=\"train\", train=True):\n",
        "        img_root  = f\"{DATA_ROOT}/{split}/images\"\n",
        "        mask_root = f\"{DATA_ROOT}/{split}/masks\"\n",
        "        img_paths = sorted(glob.glob(f\"{img_root}/*.jpg\"))\n",
        "\n",
        "        mask_paths = glob.glob(f\"{mask_root}/**/*.png\", recursive=True)\n",
        "        lookup = {os.path.splitext(os.path.basename(p))[0]: p for p in mask_paths}\n",
        "\n",
        "        self.imgs, self.masks = [], []\n",
        "        for im in img_paths:\n",
        "            key = os.path.splitext(os.path.basename(im))[0]\n",
        "            if key in lookup:\n",
        "                self.imgs.append(im)\n",
        "                self.masks.append(lookup[key])\n",
        "\n",
        "        if not self.imgs:\n",
        "            raise RuntimeError(f\"No pairs found in split '{split}'\")\n",
        "\n",
        "        self.tfm = A.Compose([\n",
        "            A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE),\n",
        "                                scale=(0.8, 1.0), ratio=(0.75, 1.33), p=1.0)\n",
        "                if train else A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "            A.HorizontalFlip(p=0.5) if train else A.NoOp(),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = cv2.cvtColor(cv2.imread(self.imgs[idx]), cv2.COLOR_BGR2RGB)\n",
        "        y = cv2.imread(self.masks[idx], cv2.IMREAD_UNCHANGED)\n",
        "        t = self.tfm(image=x, mask=y)\n",
        "        return t[\"image\"]/255.0, t[\"mask\"].long()\n",
        "\n",
        "# -------------- loaders --------------------------------------------------\n",
        "train_set = SegDataset(\"train\", train=True)\n",
        "val_set   = SegDataset(\"val\",   train=False)\n",
        "test_set  = SegDataset(\"test\",  train=False)\n",
        "\n",
        "print(f\"train {len(train_set)} | val {len(val_set)} | test {len(test_set)}\")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_set,   batch_size=4, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_set,  batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# sample shape check\n",
        "x, y = train_set[0]\n",
        "print(\"sample shapes:\", x.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4cnWk9JKPN"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu5Omx_9JUB6"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x)\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, n_classes=4, base=32):\n",
        "        super().__init__()\n",
        "        self.e1 = Block(3,      base)\n",
        "        self.e2 = Block(base,   base*2)\n",
        "        self.e3 = Block(base*2, base*4)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.mid = Block(base*4, base*8)\n",
        "        self.u2  = nn.ConvTranspose2d(base*8, base*4, 2, 2)\n",
        "        self.d2  = Block(base*8, base*4)\n",
        "        self.u1  = nn.ConvTranspose2d(base*4, base*2, 2, 2)\n",
        "        self.d1  = Block(base*4, base*2)\n",
        "        self.u0  = nn.ConvTranspose2d(base*2, base,   2, 2)\n",
        "        self.d0  = Block(base*2, base)\n",
        "        self.head = nn.Conv2d(base, n_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.e1(x)\n",
        "        e2 = self.e2(self.pool(e1))\n",
        "        e3 = self.e3(self.pool(e2))\n",
        "        m  = self.mid(self.pool(e3))\n",
        "        d2 = self.d2(torch.cat([self.u2(m), e3], 1))\n",
        "        d1 = self.d1(torch.cat([self.u1(d2), e2], 1))\n",
        "        d0 = self.d0(torch.cat([self.u0(d1), e1], 1))\n",
        "        return self.head(d0)\n",
        "\n",
        "n_classes = 4   # 0-bg + 1-Banana + 2-Orange + 3-Strawberry\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model  = TinyUNet(n_classes).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci53OBAJJicw"
      },
      "source": [
        "#Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "frLpXwXzJku5",
        "outputId": "b12b7059-ad00-485f-dde7-411db764c91d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ep 00  train 0.824  val 0.718\n",
            "ep 01  train 0.670  val 0.638\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc0d5d43d983>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrain_loss_curve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F, torch.optim as optim\n",
        "\n",
        "def dice_loss(pred, targ, eps=1e-6):\n",
        "    pred = F.softmax(pred, dim=1)\n",
        "    oh = F.one_hot(targ, n_classes).permute(0,3,1,2).float()\n",
        "    inter = (pred*oh).sum((2,3)); union = pred.sum((2,3))+oh.sum((2,3))\n",
        "    return 1 - ((2*inter+eps)/(union+eps)).mean()\n",
        "\n",
        "def loss_fn(p,t): return 0.5*F.cross_entropy(p,t)+0.5*dice_loss(p,t)\n",
        "\n",
        "opt = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "train_loss_curve, val_loss_curve = [], []\n",
        "\n",
        "for epoch in range(10):\n",
        "    # ---- train ----\n",
        "    model.train(); running = 0\n",
        "    for x,y in train_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        opt.zero_grad(); loss = loss_fn(model(x),y)\n",
        "        loss.backward(); opt.step(); running += loss.item()\n",
        "    tl = running/len(train_loader); train_loss_curve.append(tl)\n",
        "\n",
        "    # ---- val ----\n",
        "    model.eval(); running = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in val_loader:\n",
        "            running += loss_fn(model(x.to(device)), y.to(device)).item()\n",
        "    vl = running/len(val_loader); val_loss_curve.append(vl)\n",
        "\n",
        "    print(f\"ep {epoch:02d}  train {tl:.3f}  val {vl:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVgOGoZwfASs"
      },
      "outputs": [],
      "source": [
        "# ── Cell: plot loss curves -----------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(train_loss_curve) + 1)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(epochs, train_loss_curve, label=\"Train loss\")\n",
        "plt.plot(epochs, val_loss_curve, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Tiny-U-Net training curves\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKUJdSDtfXrh"
      },
      "source": [
        "#Testing and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPpzj0JwOiq1",
        "outputId": "d20fa170-cacf-4d89-9c5d-94177eb3782e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice: {'bg': np.float32(0.895), 'Cat': np.float32(0.625), 'Car': np.float32(0.277), 'Person': np.float32(0.0)}\n",
            "Macro-F1 0.300 | Micro-F1 0.427\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np, torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    inter = torch.zeros(n_classes, device=device)\n",
        "    union = torch.zeros(n_classes, device=device)\n",
        "    preds, gts = [], []\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        for c in range(n_classes):\n",
        "            pc = (p==c); yc = (y==c)\n",
        "            inter[c] += (pc & yc).sum()\n",
        "            union[c] += pc.sum() + yc.sum()\n",
        "        preds += p.view(-1).cpu().tolist()\n",
        "        gts   += y.view(-1).cpu().tolist()\n",
        "    dice = (2*inter / union.clamp(min=1)).cpu().numpy()\n",
        "    macro = f1_score(gts, preds, average=\"macro\", labels=range(1,n_classes))\n",
        "    micro = f1_score(gts, preds, average=\"micro\", labels=range(1,n_classes))\n",
        "    return dice, macro, micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyALdOKpeM0Z"
      },
      "outputs": [],
      "source": [
        "fruit_labels = [\"bg\",\"Banana\",\"Orange\",\"Strawberry\"]\n",
        "\n",
        "dice, macroF1, microF1 = evaluate()\n",
        "print(\"Dice:\", dict(zip(fruit_labels, dice.round(3))))\n",
        "print(f\"Macro-F1 {macroF1:.3f} | Micro-F1 {microF1:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
