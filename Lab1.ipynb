{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hubertas Vindžigalskis, LSP: 2213817, [\"Traffic light\", \"Sandal\", \"Castle\"]"
      ],
      "metadata": {
        "id": "1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2"
      },
      "source": [
        "# Pasiruošimas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32dbf89-99a9-474e-bbc1-6f5b32cc7da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openimages\n",
            "  Downloading openimages-0.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting boto3 (from openimages)\n",
            "  Downloading boto3-1.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting cvdata (from openimages)\n",
            "  Downloading cvdata-0.0.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from openimages) (5.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from openimages) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openimages) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openimages) (4.67.1)\n",
            "Collecting botocore<1.38.0,>=1.37.0 (from boto3->openimages)\n",
            "  Downloading botocore-1.37.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->openimages)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->openimages)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->openimages) (1.17.0)\n",
            "Downloading openimages-0.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading boto3-1.37.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cvdata-0.0.3-py3-none-any.whl (37 kB)\n",
            "Downloading botocore-1.37.0-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3, cvdata, openimages\n",
            "Successfully installed boto3-1.37.0 botocore-1.37.0 cvdata-0.0.3 jmespath-1.0.1 openimages-0.0.1 s3transfer-0.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openimages\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from openimages.download import download_dataset\n",
        "from PIL import Image\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35db7ed-5e5f-4bcc-e61e-82e5646b32ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Images already downloaded for all classes, skipping download.\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/colab_content\"\n",
        "SI_ROOT = DATA_ROOT + \"/SampleImages\"\n",
        "OI_ROOT = DATA_ROOT + \"/OpenImages\"\n",
        "SAMPLE_LIMIT = 380\n",
        "TARGET_LABELS = [\"Traffic light\", \"Sandal\", \"Strawberry\"]\n",
        "\n",
        "def dataset_exists(root, labels):\n",
        "    return all(os.path.exists(os.path.join(root, lbl.lower())) for lbl in labels)\n",
        "\n",
        "if not dataset_exists(OI_ROOT, TARGET_LABELS):\n",
        "    download_dataset(OI_ROOT, TARGET_LABELS, limit=SAMPLE_LIMIT)\n",
        "else:\n",
        "    print(\"Images already downloaded for all classes, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5"
      },
      "source": [
        "# Procesoriaus ir modelio paruošimas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ff12c2-616a-48d5-c5b1-5fb434256193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:09<00:00, 61.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = models.vgg19(pretrained=True).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11"
      },
      "source": [
        "# Dataset paruošimas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938dfbd6-a2f9-46b0-e369-7e95adaa9a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'traffic light', 1: 'sandal', 2: 'strawberry'}\n",
            "{'traffic light': 0, 'sandal': 1, 'strawberry': 2}\n"
          ]
        }
      ],
      "source": [
        "class_dirs = glob.glob(os.path.join(OI_ROOT, '*'))\n",
        "folder_names = [os.path.basename(folder) for folder in class_dirs]\n",
        "file_paths = [glob.glob(os.path.join(folder, \"images\", \"*\")) for folder in class_dirs]\n",
        "all_files = [fp for sublist in file_paths for fp in sublist]\n",
        "all_sample_files = glob.glob(os.path.join(SI_ROOT, \"*\"))\n",
        "\n",
        "idx_to_class = {i: name for i, name in enumerate(folder_names)}\n",
        "class_to_idx = {name: i for i, name in idx_to_class.items()}\n",
        "print(idx_to_class)\n",
        "print(class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, files, transform):\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fpath = self.files[index]\n",
        "        img = Image.open(fpath)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        img = self.transform(img)\n",
        "\n",
        "        # Extract label from path\n",
        "        if (os.path.basename(os.path.dirname(fpath)) == \"images\"):\n",
        "            label = os.path.basename(os.path.dirname(os.path.dirname(fpath)))\n",
        "        else:\n",
        "            fname = os.path.basename(fpath)\n",
        "            basename = os.path.splitext(fname)[0]\n",
        "            label = basename.split('_')[0].lower()\n",
        "            label = label.replace('-', ' ')\n",
        "        return img, class_to_idx[label]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "PegPPPo9T0xW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "data = Dataset(all_files, img_transform)\n",
        "loader = DataLoader(data, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# To use custom images, uncomment this section:\n",
        "# data = Dataset(all_sample_files, img_transform)\n",
        "# loader = DataLoader(data, batch_size=1, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18"
      },
      "source": [
        "# Inference ciklas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "gt_all = []      # Ground-truth labels\n",
        "pred_tl = []     # Predictions for \"Traffic light\" (using index 920)\n",
        "pred_sd = []     # Predictions for \"Sandal\" (using index 774)\n",
        "pred_sb = []     # Predictions for \"Strawberry\" (using index 483)\n",
        "\n",
        "for images, labels in loader:\n",
        "    outputs = model(images.to(device))\n",
        "    for i in range(outputs.size(0)):\n",
        "        # Softmax produces a probability distribution over multiple classes\n",
        "        # Sigmoid works on a single class at a time\n",
        "        probs = torch.softmax(outputs[i], dim=0).detach().cpu().numpy()\n",
        "        pred_tl.append(probs[920])\n",
        "        pred_sd.append(probs[774])\n",
        "        pred_sb.append(probs[949])\n",
        "    gt_all.extend(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konfuzijos matrica ir matavimai"
      ],
      "metadata": {
        "id": "20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "def display_confusion_matrix(class_idx, matrix):\n",
        "    print(\"-------------------\")\n",
        "    print(\"|   TP   |   FP   |\")\n",
        "    print(\"| {0:^6} | {1:^6} |\".format(matrix['TP'], matrix['FP']))\n",
        "    print(\"|--------|--------|\")\n",
        "    print(\"|   FN   |   TN   |\")\n",
        "    print(\"| {0:^6} | {1:^6} |\".format(matrix['FN'], matrix['TN']))\n",
        "    print(\"-------------------\")\n",
        "\n",
        "def compute_confusion_matrix(gt, pred, cls, thresh = 0.5):\n",
        "    binary_pred = (np.array(pred) >= thresh).astype(int)\n",
        "    matrix = {\n",
        "        'TP': np.sum((np.array(gt) == cls) & (binary_pred == 1)),\n",
        "        'TN': np.sum((np.array(gt) != cls) & (binary_pred == 0)),\n",
        "        'FP': np.sum((np.array(gt) != cls) & (binary_pred == 1)),\n",
        "        'FN': np.sum((np.array(gt) == cls) & (binary_pred == 0)),\n",
        "    }\n",
        "    return matrix\n",
        "\n",
        "def calculate_metrics(TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
        "\n",
        "    if (TP + FN):\n",
        "        recall = TP / (TP + FN)\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (TP + FP):\n",
        "        precision = TP / (TP + FP)\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if (recall + precision):\n",
        "        f1 = 2 * (recall * precision) / (recall + precision)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    return {'accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1}\n",
        "\n",
        "def show_metrics(mets, cid):\n",
        "    print(\"  accuracy : \", mets['accuracy'])\n",
        "    print(\"  recall : \", mets['recall'])\n",
        "    print(\"  precision : \", mets['precision'])\n",
        "    print(\"  f1 : \", mets['f1'])\n",
        "    print()\n",
        "\n",
        "def show_overall(mets):\n",
        "    print(\"  accuracy : \", mets['accuracy'])\n",
        "    print(\"  recall : \", mets['recall'])\n",
        "    print(\"  precision : \", mets['precision'])\n",
        "    print(\"  f1 : \", mets['f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atliekame skaičiavimus"
      ],
      "metadata": {
        "id": "24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_tl = compute_confusion_matrix(gt_all, pred_tl, 0, thresh=0.05)  # For \"Traffic light\"\n",
        "conf_sd = compute_confusion_matrix(gt_all, pred_sd, 1, thresh=0.05)  # For \"Sandal\"\n",
        "conf_sb = compute_confusion_matrix(gt_all, pred_sb, 2, thresh=0.05)  # For \"Strawberry\"\n",
        "\n",
        "metrics_tl = calculate_metrics(conf_tl['TP'], conf_tl['TN'], conf_tl['FP'], conf_tl['FN'])\n",
        "metrics_sd = calculate_metrics(conf_sd['TP'], conf_sd['TN'], conf_sd['FP'], conf_sd['FN'])\n",
        "metrics_sb = calculate_metrics(conf_sb['TP'], conf_sb['TN'], conf_sb['FP'], conf_sb['FN'])\n",
        "\n",
        "combined_conf = {k: conf_tl[k] + conf_sb[k] + conf_sd[k] for k in ['TP','TN','FP','FN']}\n",
        "metrics_all = calculate_metrics(combined_conf['TP'], combined_conf['TN'], combined_conf['FP'], combined_conf['FN'])\n",
        "\n",
        "print(\"Class \", idx_to_class[0], \" metrics:\")\n",
        "display_confusion_matrix(0, conf_tl)\n",
        "show_metrics(metrics_tl, 0)\n",
        "print(\"Class \", idx_to_class[1], \" metrics:\")\n",
        "display_confusion_matrix(1, conf_sd)\n",
        "show_metrics(metrics_sd, 1)\n",
        "print(\"Class \", idx_to_class[2], \" metrics:\")\n",
        "display_confusion_matrix(2, conf_sb)\n",
        "show_metrics(metrics_sb, 2)\n",
        "print(\"All  metrics:\")\n",
        "show_overall(metrics_all)"
      ],
      "metadata": {
        "id": "25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb74870-c0f4-45bc-8a04-85fc3173a1f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class  traffic light  metrics:\n",
            "-------------------\n",
            "|   TP   |   FP   |\n",
            "|  344   |   0    |\n",
            "|--------|--------|\n",
            "|   FN   |   TN   |\n",
            "|   36   |  681   |\n",
            "-------------------\n",
            "  accuracy :  0.9660697455230914\n",
            "  recall :  0.9052631578947369\n",
            "  precision :  1.0\n",
            "  f1 :  0.9502762430939227\n",
            "\n",
            "Class  sandal  metrics:\n",
            "-------------------\n",
            "|   TP   |   FP   |\n",
            "|  155   |   1    |\n",
            "|--------|--------|\n",
            "|   FN   |   TN   |\n",
            "|  146   |  759   |\n",
            "-------------------\n",
            "  accuracy :  0.8614514608859567\n",
            "  recall :  0.5149501661129569\n",
            "  precision :  0.9935897435897436\n",
            "  f1 :  0.6783369803063457\n",
            "\n",
            "Class  strawberry  metrics:\n",
            "-------------------\n",
            "|   TP   |   FP   |\n",
            "|  255   |   0    |\n",
            "|--------|--------|\n",
            "|   FN   |   TN   |\n",
            "|  125   |  681   |\n",
            "-------------------\n",
            "  accuracy :  0.882186616399623\n",
            "  recall :  0.6710526315789473\n",
            "  precision :  1.0\n",
            "  f1 :  0.8031496062992126\n",
            "\n",
            "All  metrics:\n",
            "  accuracy :  0.9032359409362237\n",
            "  recall :  0.7106503298774741\n",
            "  precision :  0.9986754966887417\n",
            "  f1 :  0.8303964757709251\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}